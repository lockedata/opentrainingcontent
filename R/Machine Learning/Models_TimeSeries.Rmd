
# Time Series Analysis
```{r  updatesettings2, include=FALSE}
knitr::opts_chunk$set( tidy=TRUE, results="hold", cache=TRUE, message = FALSE) 
library(tidyverse)
library(anomalize)
library(fable)
library(ggseas)
library(coindeskr)
library(lubridate)
library(tsibble)
```

# Anomaly Detection 

## Bitcoin price anomaly detection 

```{r btc}
btc <- get_historic_price(start = "2017-01-01")
```

For Anomaly Detection using `anomalize`, we need to have either a `tibble` or `tibbletime` object. Hence we have to convert the dataframe `btc` into a tibble object that follows a time series shape and store it in `btc_ts`

```{r btc_ts}
btc_ts <- btc %>% rownames_to_column() %>% as.tibble() %>% 
  mutate(date = as.Date(rowname)) %>% dplyr::select(-one_of('rowname'))
``` 

One of the important things to do with Time Series data before starting with Time Series forecasting or Modelling is Time Series Decomposition where the Time series data is decomposed into Seasonal, Trend and remainder components. anomalize has got a function time_decompose() to perform the same. Once the components are decomposed, anomalize can detect and flag anomalies in the decomposed data of the reminder component which then could be visualized with plot_anomaly_decomposition() .

```{r btc_plot}
btc_ts %>% 
  time_decompose(Price, method = "stl", frequency = "auto", trend = "auto") %>%
  anomalize(remainder, method = "gesd", alpha = 0.05, max_anoms = 0.2) %>%
  plot_anomaly_decomposition()
```

As you can see from the above code, the decomposition happens based on ‘stl’ method which is the common method of time series decomposition but if you have been using Twitter’s AnomalyDetection, then the same can be implemented in anomalize by combining time_decompose(method = “twitter”) with anomalize(method = "gesd"). Also the ‘stl’ method of decomposition can also be combined with anomalize(method = "iqr") for a different IQR based anomaly detection.

Anomaly Detection and Plotting the detected anomalies are almost similar to what we saw above with Time Series Decomposition. It’s just that decomposed components after anomaly detection are recomposed back with time_recompose() and plotted with plot_anomalies() . The package itself automatically takes care of a lot of parameter setting like index, frequency and trend, making it easier to run anomaly detection out of the box with less prior expertise in the same domain.

```{r btc_anom}
btc_ts %>% 
  time_decompose(Price) %>%
  anomalize(remainder) %>%
  time_recompose() %>%
  plot_anomalies(time_recomposed = TRUE, ncol = 3, alpha_dots = 0.5)
```

It could be very well inferred from the given plot how accurate the anomaly detection is finding out the Bitcoin Price madness that happened during the early 2018.
If you are interested in extracting the actual datapoints which are anomalies, the following code could be used:

```{r btc_extract}
btc_ts %>% 
  time_decompose(Price) %>%
  anomalize(remainder) %>%
  time_recompose() %>%
  filter(anomaly == 'Yes') 
```

# Forecasting

The `fable` package for doing tidy forecasting in R is now on CRAN. Like `tsibble` and `feasts`, it is also part of the `tidyverts` family of packages for analysing, modelling and forecasting many related time series (stored as `tsibbles`).

Here we will forecast Australian tourism data by state/region and purpose. This data is stored in the `tourism` tsibble where Trips contains domestic visitor nights in thousands.

There are 304 combinations of Region, State and Purpose, each one defining a time series of 80 observations.

To simplify the outputs, we will abbreviate the state names.

```{r simplify}
tourism <- tourism %>%
  mutate(
    State = dplyr::recode(State,
      "Australian Capital Territory" = "ACT",
      "New South Wales" = "NSW",
      "Northern Territory" = "NT",
      "Queensland" = "QLD",
      "South Australia" = "SA",
      "Tasmania" = "TAS",
      "Victoria" = "VIC",
      "Western Australia" = "WA"
    )
  )
```

## Forecasting a single time series 

Although the fable package is designed to handle many time series, we will be begin by demonstrating its use on a single time series. For this purpose, we will extract the tourism data for holidays in the Snowy Mountains region of NSW.

```{r snowy}
snowy <- tourism %>%
  filter(
    Region == "Snowy Mountains",
    Purpose == "Holiday"
  )
snowy
```

```{r snowy_trips, eval=FALSE}
autoplot(snowy, Trips)
```

For this data set, a reasonable benchmark forecast method is the seasonal naive method, where forecasts are set to be equal to the last observed value from the same quarter. Alternative models for this series are ETS and ARIMA models. All these can be included in a single call to the model() function like this.

```{r snowy_fit}
fit <- snowy %>%
  model(
    snaive = SNAIVE(Trips ~ lag("year")),
    ets = ETS(Trips),
    arima = ARIMA(Trips)
  )
fit
```

The returned object is called a “mable” or model table, where each cell corresponds to a fitted model. Because we have only fitted models to one time series, this mable has only one row.

To forecast all models, we pass the object to the forecast function.

```{r snowy_model}
fc <- fit %>%
  forecast(h = 12)
fc
```

The return object is a “fable” or forecast table with the following characteristics:

the `.model` column becomes an additional key;
the `.distribution` column contains the estimated probability distribution of the response variable in future time periods;
the `Trips` column contains the point forecasts equal to the mean of the probability distribution.
The `autoplot()` function will produce a plot of all forecasts. By default, `level=c(80,95)` so 80% and 95% prediction intervals are shown. But to avoid clutter, we will set `level=NULL` to show no prediction intervals.

```{r snowy_fit2}
fc %>%
  autoplot(snowy, level = NULL) +
  ggtitle("Forecasts for Snowy Mountains holidays") +
  xlab("Year") +
  guides(colour = guide_legend(title = "Forecast"))
```

If you want to compute the prediction intervals, the hilo() function can be used:

```{r snowy_hilo}
hilo(fc, level = 95)
```

### Exercise

Re-run this for Melbourne Business Trips


## Forecasting many series 

To scale this up to include all series in the tourism data set requires no more work — we use exactly the same code.

```{r tourism_trunc, echo=FALSE}
tourism <- filter(tourism,Region == "Snowy Mountains")
```

```{r tourism_model}
fit <- tourism %>%
  model(
    snaive = SNAIVE(Trips ~ lag("year")),
    ets = ETS(Trips),
    arima = ARIMA(Trips)
  )
fit
```


Now the `mable` includes models for every combination of keys in the `tourism` data set.

We can extract information about some specific model using the `filter`, `select` and `report` functions.

```{r tourism_extract}
fit %>%
  filter(Region == "Snowy Mountains", Purpose == "Holiday") %>%
  dplyr::select(arima) %>%
  report()
```

When the mable is passed to the forecast() function, forecasts are computed for every model and every key combination.

```{r tourism_forecast}
fc <- fit %>%
  forecast(h = "3 years")
fc
```

Note the use of natural language to specify the forecast horizon. The forecast() function is able to interpret many different time specifications. For quarterly data, h = "3 years" is equivalent to setting h = 12.

Plots of individual forecasts can also be produced, although filtering is helpful to avoid plotting too many series at once.

```{r tourism_plots}
fc %>%
  filter(Region == "Snowy Mountains") %>%
  autoplot(tourism, level = NULL) +
  xlab("Year") + ylab("Overnight trips (thousands)")
```

### Exercise
Perform a 5 year forecast

## Forecast accuracy calculations 

To compare the forecast accuracy of these models, we will create a training data set containing all data up to 2014. We will then forecast the remaining years in the data set and compare the results with the actual values.

```{r tourism_train}
train <- tourism %>%
  filter(year(Quarter) <= 2014)
fit <- train %>%
  model(
    ets = ETS(Trips),
    arima = ARIMA(Trips),
    snaive = SNAIVE(Trips)
  ) %>%
  mutate(mixed = (ets + arima + snaive) / 3)
```

Here we have introduced an ensemble forecast (mixed) which is a simple average of the three fitted models. Note that forecast() will produce distributional forecasts from the ensemble as well, taking into account the correlations between the forecast errors of the component models.

```{r tourism_forecast_fit}
fc <- fit %>% forecast(h = "3 years")
fc %>%
  filter(Region == "Snowy Mountains") %>%
  autoplot(tourism, level = NULL)
```


Now to check the accuracy, we use the accuracy() function. By default it computes several point forecasting accuracy measures such as MAE, RMSE, MAPE and MASE for every key combination.

```{r tourism_accuracy}
accuracy(fc, tourism)
```

But because we have generated distributional forecasts, it is also interesting to look at the accuracy using CRPS (Continuous Rank Probability Scores) and Winkler Scores (for 95% prediction intervals).

```{r tourism_CRPS}
fc_accuracy <- accuracy(fc, tourism,
  measures = list(
    point_accuracy_measures,
    interval_accuracy_measures,
    distribution_accuracy_measures
  )
)
```

```{r tourism_acc_RMSE}
fc_accuracy %>%
  group_by(.model) %>%
  summarise(
    RMSE = mean(RMSE),
    MAE = mean(MAE),
    MASE = mean(MASE),
    Winkler = mean(winkler),
    CRPS = mean(CRPS)
  ) %>%
  arrange(RMSE)
```

### Exercise
Which model is doing best on all accuracy measures? 

Re-run this using the Kangaroo Island Region and see how the models change 

# Time series visualisations 

The `ggseas()` package aims to help exploratory analysis of time series by making it easy to do seasonal adjustments and decomposition on the fly. 

It provides two main sets of functionality:

a collection of ggplot2 stats oriented to time-series, allowing indexing, rolling averages and seasonal decomposition to be added as straight-forward statistical transforms with familiar geoms like geom_line and geom_point.
the ggsdc() function which provides decomposition similar to that from decompose, stl or the X13-SEATS-ARIMA world via the seasonal package, except in ggplot2 terms so you can use familiar themes, titles, control your geoms and scales, etc. - and have multiple time series decomposed on a single graphic.

## Using stats for simple time series graphic composition

`ggseas` provides five stats that have `geom_line` as their default geom and will easily integrate into a time series exploration:

`stat_index`
`stat_decomp`
`stat_rollapplyr`
`stat_stl`
`stat_seas`

Like everything in the ggplot2 universe, these need your data to be in a data.frame (or its modern cousin the tibble). Much time series analysis in R uses ts and mts objects rather than data.frames so a first step to use ggplot2 for time series is to convert your data into a data.frame. The very simple tsdf function is provided to make this easier (there must be someone who’s done this earlier and better than me, if so let me know). It extracts the information on time from a ts or mts object and makes it a column, with time series data columns to the right of it:

```{r ggseas}
library(ggseas)
ap_df <- tsdf(AirPassengers)
head(ap_df)
```

This means we can draw our time series in the usual ggplot2 way:

```{r ggseas_simple}
ggplot(ap_df, aes(x = x, y = y)) + 
   geom_line(colour = "grey75") 
```

## Convert your data to an index with stat_index()

One thing we often want to do with time series data is turn it into an index with some arbitrarily chosen point as a reference, often set to equal 100. The stat_index() stat does this for you. In the case below, the average value of the first 10 points of the data is chosen as the reference to equal the default of 100.

```{r ggseas_index}
ggplot(ap_df, aes(x = x, y = y)) + 
   stat_index(index.ref = 1:10)
```

You can control all that. For example, you might want the 120th point of the data to be equal to 1000

```{r ggseas_points}
ggplot(ap_df, aes(x = x, y = y)) + 
   stat_index(index.ref = 120, index.basis = 1000)
```

### Exercise

Play around with the midpoint value to see how your plot changes

Indexing also turns up later as an option in most of the other functions in ggseas.

### Make rolling averages easily with stat_rollapplyr()

`stat_rollapplyr()` makes it easy to add a rolling function to your data, defaulting to the mean:

```{r ggseas roll}
ggplot(ap_df, aes(x = x, y = y)) + 
   geom_line(colour = "grey75") +
   stat_rollapplyr(width = 12, align = "center") +
   labs(x = "", y = "Number of US Air Passengers\n(rolling average and original)")
```

It’s not very likely you’ll want anything but a line for a graphic like this, but in case you do, you can over-write the default geom. And of course all the usual ggplot2 polishing facilities are available:

```{r ggseas defaults}
ggplot(ap_df, aes(x = x, y = y)) + 
   geom_line(colour = "grey75") +
   stat_rollapplyr(width = 12, align = "center", geom = "point", 
                   size = 0.5, colour = "steelblue") +
   labs(x = "", y = "Number of US Air Passengers\n(rolling average and original)")
```

## Seasonal adjustment

Three stats will do seasonal adjustment for you, but I recommend only two of them: `stat_stl()` and `stat_seas()`. In fact, if `stat_seas` works it will generally be better. Amongst other things it does automatic detection of outliers and level shifts, best transformation to make, and adjustments for Easter and number of trading days. It’s driven by Christopher Sax’s `seasonal` R package.

```{r ggseas stl}
ggplot(ap_df, aes(x = x, y = y)) +
   geom_line(colour = "grey50") +
   stat_seas(colour = "blue") +
   stat_stl(s.window = 7, colour = "red")
```

## Leverage ggplot2 and seasonal adjustment

One of the more interesting implications of bringing seasonal adjustment into the ggplot2 universe is the ability to slice and dice the data by aesthetic mappings (usually to colour) by faceting. This all works seamlessly in a way that will be familiar to ggplot2 users eg

```{r ggseas facet}
ggplot(ldeaths_df, aes(x = YearMon, y = deaths, colour = sex)) +
  geom_point() +
  facet_wrap(~sex) +
  stat_seas() +
  ggtitle("Seasonally adjusted lung deaths in the UK")
```

## ggsdc for quick decomposition into trend, seasonal and random components

The final key functionality from the ggseas package is the ability to do decomposition into trend, seasonal and random components, within the ggplot paradigm for polishing, aesthetic mappings, etc. Unlike the stat_seas() and its cousins which are stats that fit into the usual ggplot() + ... +... pipeline, ggsdc() is a replacement for the ggplot() function that normally starts that pipeline. ggsdc() has to seize control of the faceting operations in the pipeline and a few other things so it can’t be added just as a geom (at least not to this author’s understanding).

ggsdc() provides access to classic decomposition (additive or multiplicative) from decompose(), loess-based via stl(), or X13-SEATS-ARIMA via seasonal(). Again, I recommend one of the latter two as the more powerful and flexible approaches to seasonality, much better at handling slowly changing seasonality over time.

Here’s how it works at its simplest:

```{r ggseas ggsdc}
ggsdc(ap_df, aes(x = x, y = y), method = "seas") + geom_line()
```

Note that you still need to add a geom (usually geom_line()) to tell ggplot2 how to render the chart

Here’s a slightly more complex version with a multivariate series:

```{r ggseas serv}
serv <- subset(nzbop, Account == "Current account" & 
                  Category %in% c("Services; Exports total", "Services; Imports total"))
ggsdc(serv, aes(x = TimePeriod, y = Value, colour = Category),
         method = "stl", s.window = 7, frequency = 4,
         facet.titles = c("The original series", "The underlying trend", 
                          "Regular seasonal patterns", "All the randomness left")) +
      geom_line()
```

### Exercise

1. Load the `nzbop` data and create a new subset for "Financial Account" and choose two new cateogries. 
2. Add to the `geom_line()` properties to change the line colours to two of your choice. 

