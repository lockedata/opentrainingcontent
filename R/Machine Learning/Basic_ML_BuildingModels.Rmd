# Building models

## tidymodels
We've so far seen a couple of packages like recipes and rsample. These are part of a growing group of packages aimed at making modelling easier and more powerful.

- broom
- infer
- recipes
- rsample
- tidytext
- tidypredict
- tidyposterior

It dovetails neatly with the tidyverse to enable powerful capabilities with few lines of code. 

In roughly 20 lines of code, we;re able to load data, perform cross-validation, produce a model per sample, and visualise the distribution of coefficients that resulted from the many models.

```{r Basic_ML_BuildingModels-1, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
predictors %>% 
  cbind(diagnosis) ->
  alzheimers

alzheimers %>% 
  rsample::vfold_cv() ->
  alz_cv

alz_cv  %>% 
  pluck("splits") %>% 
  map(as_data_frame) %>% 
  map(~lm(diagnosis ~ ., data=.)) %>% 
  map_df(broom::tidy, .id = "cv") %>% 
  arrange(term) %>% 
  slice(1:60) %>% 
  ggplot() + 
  aes(x=estimate) +
  geom_density() +
  facet_wrap(~term, scales = "free")
```

We can use these packages to build a range of models quickly and effectively.

# Candidate models
Expect more capabilities soon in this area. Make sure to check out the infer, tidytext, tidybayes, and the tidyverts for modelling techniques.

## All variables
Use all variables to get a view of how much accuracy can be achieved with the greatest likelihood of being overfit. In practice, you won't select this model but it gives you important context about the upper bound of accuracy with your current variables.

```{r Basic_ML_BuildingModels-2}
alz_s = initial_split(alzheimers)
alz_train = training(alz_s)
alz_test = testing(alz_s)

recipe(diagnosis ~ ., alz_train) %>% 
  step_scale(all_numeric()) %>% 
  step_pca(all_numeric()) %>% 
  step_dummy(all_outcomes()) %>% 
  prep(alz_train) ->
    rec

alz_train %>% 
  bake(rec, .) %>% 
  glm(diagnosis_Control ~ ., data = .) %>% 
  glance()
```

## A few strong variables
Create model with only a few variables. Select the ones that have the most predictive power. A simple model is easy to implement and understand so if this turns out to be "good enough" you could implement it. It also provides a lower bound for accuracy.

```{r Basic_ML_BuildingModels-3}
alz_train %>% 
  bake(rec, .) %>% 
  glm(diagnosis_Control ~ Genotype + PC1, data = .) %>% 
  glance()
```


## Other models
You would also try to consider a "common-sense" model where you select features based on experience. You would also do other types of models that would perform a classification.

## Exercise
1. Using `titanic3` create an all variables model
2. Create a "Women and children first!" model
4. Using your knowledge and the info about variable importance, make your own model

# Tuning models 

Tuning a model often requires exploring the impact of changes to many hyperparameters. When creating a machine learning model, you'll be presented with design choices as to how to define your model architecture. Often times, we don't immediately know what the optimal model architecture should be for a given model, and thus we'd like to be able to explore a range of possibilities. In true machine learning fashion, we'll ideally ask the machine to perform this exploration and select the optimal model architecture automatically. Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model architecture is referred to as hyperparameter tuning.


Keep in mind though, that hyper-parameter tuning can only improve the model so much without overfitting. If you canâ€™t achieve sufficient accuracy, the input features might simply not be adequate for the predictions you are trying to model. It might be necessary to go back to the original features and try e.g. feature engineering methods.

These hyperparameters might address model design questions such as:

- What degree of polynomial features should I use for my linear model?
- What should be the maximum depth allowed for my decision tree?
- What should be the minimum number of samples required at a leaf node in my decision tree?
- How many trees should I include in my random forest?
- How many neurons should I have in my neural network layer?
- How many layers should I have in my neural network?
- What should I set my learning rate to for gradient descent?

There are several approaches to hyperparameter tuning

- Manual: select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat process until you run out of patience or are satisfied with the results.
- Grid Search: set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameters values is tried which can be very inefficient!
- Random search: set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources.
- Automated Hyperparameter Tuning: use methods such as gradient descent, Bayesian Optimization, or evolutionary algorithms to conduct a guided search for the best hyperparameters.

https://www.h2o.ai/blog/h2o-gbm-tuning-tutorial-for-r/

https://shiring.github.io/machine_learning/2017/03/07/grid_search

## Grid Search

Grid search involves running a model many times with combinations of various hyperparameters. The point is to identify which hyperparameters are likely to work best.

## Tuning Machine Learning Models  using {tidymodels} (Experimental/development)

Machine learning algorithms are parameterized so that they can be best adapted for a given problem. A difficulty is that configuring an algorithm for a given problem can be a project in and of itself.


### Model Tuning
{tune} provides a grid search where it or you can specify the parameters to try on your problem. It will trial all combinations and locate the one combination that gives the best results.

To demonstrate model tuning, we'll use the Ionosphere data in the `mlbench` package:

```{r load-data}
library(tune)
library(mlbench)
data(Ionosphere)
Ionosphere <- Ionosphere %>% dplyr::select(-V2)
```


There are 43 predictors and a factor outcome. Two of the predictors are factors (`V1` and `V2`) and the rest are numerics that have been scaled to a range of -1 to 1. `V2` has zero variance so can be dropped.

We'll fit a radial basis function support vector machine to these data and tune the SVM cost parameter and the $\sigma$ parameter in the kernel function:

```{r svm-mod}
svm_mod <-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```


```{r rec}
iono_rec <-
  recipe(Class ~ ., data = Ionosphere)  %>%
  # In case V1 is has a single value sampled
  step_zv(all_predictors()) %>% 
  # convert it to a dummy variable
  step_dummy(V1) %>%
  # Scale it the same as the others
  step_range(matches("V1_"))
```

The only other required item for tuning is a resampling strategy as defined by an `rsample` object. Let's demonstrate using basic bootstrapping:

```{r rs}
iono_rs <- bootstraps(Ionosphere, times = 30)
```

## Optional Inputs

An _optional_ step for model tuning is to specify which metrics should be computed using the out-of-sample predictions. For classification, the default is to calculate the log-likelihood statistic and overall accuracy. Instead of the defaults, the area under the ROC curve will be used. To do this, a `yardstick` function can be used to create a metric set:

```{r roc}
roc_vals <- metric_set(roc_auc)
```

A grid can be given in a data frame where the parameters are in columns and parameter combinations are in rows. A control object can be passed that specifies different aspects of the search. Here, the verbose option is turned off. 

```{r ctrl}
ctrl <- control_grid(verbose = FALSE)
grid_form <-
  tune_grid(
    Class ~ .,
    model = svm_mod,
    resamples = iono_rs,
    metrics = roc_vals,
    control = ctrl
  )
grid_form
```